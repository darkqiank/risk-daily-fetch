目标代码：
from bs4 import BeautifulSoup
import scrapy


class DefaultSpider(scrapy.Spider):
    name = 'template_bs'
    custom_settings = {
        "DOWNLOAD_HANDLERS": {
            "http": "scrapy_impersonate.ImpersonateDownloadHandler",
            "https": "scrapy_impersonate.ImpersonateDownloadHandler",
        },
        "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
    }

    def start_requests(self):
        url = ''
        yield scrapy.Request(url, dont_filter=True,
                meta={"impersonate": "chrome"},)

    def parse(self, response):
        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.find_all('div', class_="blog-card-cta")

        # 打印所有链接
        for item in items:
            a = item.findNext('a')
            link = f"{a['href']}"
            yield {'link': link, 'source': self.name}

源代码：
from curl_cffi import requests
from bs4 import BeautifulSoup


def get_links():
    # 目标URL
    url = 'https://ti.dbappsecurity.com.cn/blog/'

    # 发送HTTP请求
    response = requests.get(url, impersonate="chrome")
    response.encoding = 'utf-8'  # 设置编码

    # print(response.text)
    # 解析HTML
    soup = BeautifulSoup(response.text, 'html.parser')

    # 查找所有文章链接
    items = soup.find_all('h2', class_="entry-title")

    links = []

    # 打印所有链接
    for item in items:
        a = item.findNext('a')
        links.append(a['href'])

    print(links)
    return links


请把源代码转为目标代码的格式